"""
RDF/Turtle export functionality using Wikidata's full RDF structure.

Exports query results as RDF triples with complete provenance tracking.
"""

from typing import Optional, Dict, Any
import polars as pl
from rdflib import Graph, Namespace, Literal, URIRef, BNode
from rdflib.namespace import RDF, RDFS, XSD, DCTERMS

from ..core.config import CONFIG, WIKIDATA_URL, WIKIDATA_WIKI_URL
from ..core.constants import WD, WDREF, WDS, WDT, P, PS, PR, PROV, SCHEMA
from .metadata import create_dataset_hashes

__all__ = [
    "export_to_rdf_turtle",
    "build_dataset_description",
]


def build_dataset_description(
    taxon_input: str, filters: Optional[Dict[str, Any]]
) -> tuple[str, str]:
    """
    Build descriptive dataset name and description.

    Args:
        taxon_input: Taxon name or identifier
        filters: Active filters dictionary

    Returns:
        Tuple of (dataset_name, dataset_description)
    """
    dataset_name = f"LOTUS Data for {taxon_input}"
    dataset_desc = f"Chemical compounds found in taxon {taxon_input} from Wikidata"

    if filters:
        smiles_info = filters.get("chemical_structure", {})
        if smiles_info:
            search_type = smiles_info.get("search_type", "substructure")
            dataset_desc += f" using {search_type} search"

        if filters.get("mass"):
            mass = filters["mass"]
            dataset_desc += (
                f", mass filter: {mass.get('min', 'N/A')}-{mass.get('max', 'N/A')} Da"
            )

        if filters.get("molecular_formula"):
            dataset_desc += ", molecular formula constraints applied"

    return dataset_name, dataset_desc


def _add_optional_literal(
    g: Graph, subject: URIRef, predicate: URIRef, value: Any, datatype=XSD.string
) -> None:
    """Add optional literal triple to graph if value exists."""
    if value is not None and value != "":
        g.add((subject, predicate, Literal(value, datatype=datatype)))


def _add_dataset_metadata(
    g: Graph,
    dataset_uri: URIRef,
    dataset_name: str,
    dataset_desc: str,
    qid: str,
    df_len: int,
    query_hash: str,
    result_hash: str,
) -> None:
    """Add core dataset metadata to RDF graph (mutates graph in-place)."""
    # Dataset type and basic metadata
    g.add((dataset_uri, RDF.type, SCHEMA.Dataset))
    g.add((dataset_uri, SCHEMA.name, Literal(dataset_name, datatype=XSD.string)))
    g.add((dataset_uri, SCHEMA.description, Literal(dataset_desc, datatype=XSD.string)))

    # License and provenance - CC0 from Wikidata/LOTUS
    g.add(
        (
            dataset_uri,
            SCHEMA.license,
            URIRef("https://creativecommons.org/publicdomain/zero/1.0/"),
        )
    )
    g.add((dataset_uri, SCHEMA.provider, URIRef(CONFIG["app_url"])))
    g.add((dataset_uri, DCTERMS.source, URIRef(WIKIDATA_URL)))

    # Dataset statistics and versioning
    g.add((dataset_uri, SCHEMA.numberOfRecords, Literal(df_len, datatype=XSD.integer)))
    g.add(
        (
            dataset_uri,
            SCHEMA.version,
            Literal(CONFIG["app_version"], datatype=XSD.string),
        )
    )

    # Reference to LOTUS Initiative (Q104225190) as the source project
    g.add((dataset_uri, SCHEMA.isBasedOn, URIRef(WIKIDATA_WIKI_URL + "Q104225190")))

    # Link to the taxon being queried (if specific)
    if qid and qid != "*":
        g.add((dataset_uri, SCHEMA.about, WD[qid]))

    # Add provenance hashes for reproducibility
    # Query hash stored as additional metadata (how dataset was generated)
    g.add(
        (
            dataset_uri,
            DCTERMS.provenance,
            Literal(f"Generated by query with hash: {query_hash}", datatype=XSD.string),
        )
    )
    # Result hash is implicit in the dataset URI itself (urn:hash:sha256:RESULT_HASH)
    # but we also add it explicitly for clarity
    g.add(
        (
            dataset_uri,
            DCTERMS.identifier,
            Literal(f"sha256:{result_hash}", datatype=XSD.string),
        )
    )


def _add_compound_triples(
    g: Graph,
    row: Dict[str, Any],
    dataset_uri: URIRef,
    processed_taxa: set,
    processed_refs: set,
) -> None:
    """Add all triples for a single compound using Wikidata's full RDF structure."""
    compound_qid = row.get("compound_qid", "")
    if not compound_qid:
        return

    compound_uri = WD[compound_qid]

    # Link compound to dataset
    g.add((dataset_uri, SCHEMA.hasPart, compound_uri))

    # Compound identifiers using Wikidata properties (direct properties)
    _add_optional_literal(
        g, compound_uri, WDT.P235, row.get("compound_inchikey")
    )  # InChIKey
    _add_optional_literal(
        g, compound_uri, WDT.P233, row.get("compound_smiles")
    )  # Canonical SMILES
    _add_optional_literal(
        g, compound_uri, WDT.P274, row.get("molecular_formula")
    )  # Molecular formula

    # Mass (P2067)
    if row.get("compound_mass") is not None:
        _add_optional_literal(
            g, compound_uri, WDT.P2067, row["compound_mass"], XSD.float
        )

    # Compound label
    _add_optional_literal(g, compound_uri, RDFS.label, row.get("compound_name"))

    # Taxonomic association using P703 (found in taxon) with FULL STATEMENT STRUCTURE
    taxon_qid = row.get("taxon_qid")
    ref_qid = row.get("reference_qid")
    statement_uri = row.get("statement")
    ref_uri_str = row.get("ref")

    if taxon_qid:
        taxon_uri = WD[taxon_qid]

        # Use actual statement URI from Wikidata if available, otherwise use blank node
        if statement_uri and statement_uri.strip():
            statement_node = URIRef(statement_uri)
        else:
            statement_node = BNode()

        # Full statement pattern (following Wikidata RDF structure)
        g.add((compound_uri, P.P703, statement_node))  # compound has a P703 statement
        g.add((statement_node, PS.P703, taxon_uri))  # statement value is the taxon

        # Add provenance if reference exists
        if ref_qid:
            ref_uri = WD[ref_qid]

            # Use actual reference URI from Wikidata if available, otherwise use blank node
            if ref_uri_str and ref_uri_str.strip():
                ref_node = URIRef(ref_uri_str)
            else:
                ref_node = BNode()

            # Link statement to reference via provenance
            g.add((statement_node, PROV.wasDerivedFrom, ref_node))

            # Reference stated in (pr:P248)
            g.add((ref_node, PR.P248, ref_uri))

            # Add reference metadata once per unique reference
            if ref_qid not in processed_refs:
                # P1476: title
                _add_optional_literal(g, ref_uri, WDT.P1476, row.get("reference_title"))
                _add_optional_literal(
                    g, ref_uri, RDFS.label, row.get("reference_title")
                )

                # P356: DOI
                if row.get("reference_doi"):
                    _add_optional_literal(
                        g, ref_uri, WDT.P356, row.get("reference_doi")
                    )

                # P577: publication date
                if row.get("reference_date"):
                    _add_optional_literal(
                        g, ref_uri, WDT.P577, str(row["reference_date"]), XSD.date
                    )

                processed_refs.add(ref_qid)

        # Also add the simplified direct triple for convenience (wdt: namespace)
        g.add((compound_uri, WDT.P703, taxon_uri))

        # Add taxon metadata once per unique taxon
        if taxon_qid not in processed_taxa:
            # P225: taxon name
            _add_optional_literal(g, taxon_uri, WDT.P225, row.get("taxon_name"))
            _add_optional_literal(g, taxon_uri, RDFS.label, row.get("taxon_name"))
            processed_taxa.add(taxon_qid)


def export_to_rdf_turtle(
    df: pl.DataFrame,
    taxon_input: str,
    qid: str,
    filters: Optional[Dict[str, Any]] = None,
) -> str:
    """
    Export data to RDF Turtle format using Wikidata's full RDF structure.

    Args:
        df: Results dataframe
        taxon_input: Original taxon input string
        qid: Wikidata QID of taxon
        filters: Active filters dictionary

    Returns:
        RDF Turtle serialization as string
    """
    # Initialize graph
    g = Graph()

    # Bind namespaces
    g.bind("wd", WD)
    g.bind("wdref", WDREF)
    g.bind("wds", WDS)
    g.bind("wdt", WDT)
    g.bind("p", P)
    g.bind("ps", PS)
    g.bind("pr", PR)
    g.bind("prov", PROV)
    g.bind("schema", SCHEMA)
    g.bind("rdfs", RDFS)
    g.bind("xsd", XSD)
    g.bind("dcterms", DCTERMS)

    # Create dataset URI with provenance hashes
    query_hash, result_hash = create_dataset_hashes(qid, taxon_input, filters, df)
    dataset_uri = URIRef(f"urn:hash:sha256:{result_hash}")

    # Build dataset description
    dataset_name, dataset_desc = build_dataset_description(taxon_input, filters)

    # Add dataset metadata
    _add_dataset_metadata(
        g,
        dataset_uri,
        dataset_name,
        dataset_desc,
        qid,
        len(df),
        query_hash,
        result_hash,
    )

    # Track unique entities to avoid redundant triples (efficiency)
    processed_taxa = set()
    processed_refs = set()

    # Add compound data
    for row in df.iter_rows(named=True):
        _add_compound_triples(g, row, dataset_uri, processed_taxa, processed_refs)

    # Serialize to Turtle format
    return g.serialize(format="turtle")
